= Course: Model Quantization with LLM Compressor
:imagesdir: ../assets/images
:toc: left
:toclevels: 3
:sectnums:

== Introduction to Model Quantization

Welcome to the course on Model Quantization. This is one of the most impactful optimization techniques in your LLMOps toolkit. This course teaches you how to compress LLM weights to dramatically reduce memory requirements and inference costs while preserving model quality.

Quantization is transformative because it addresses the massive size of modern LLMs. By reducing the numerical precision of a model's weights (e.g., from 16-bit to 4-bit), you can achieve a 50-75% memory reduction, enabling deployment on smaller hardware and unlocking significant cost savings.

=== What You Will Learn
* **Quantization Fundamentals**: Understand the trade-offs between precision, model size, and accuracy.
* **Hands-on Techniques**: Use **LLM Compressor** with methods like **SmoothQuant** and **GPTQ** to perform W4A16 (4-bit weight, 16-bit activation) quantization.
* **Pipeline Automation**: Convert the manual process into an automated, repeatable workflow using Kubeflow Pipelines on OpenShift AI.

// -- This is the contextual narrative section to be inserted after the intro --
== Your Place in the Adventure

Welcome, Optimization Specialist! As you dive into the powerful world of quantization, let's frame this skill within your larger LLMOps adventure. You are learning to master the "AI Factory"—a system where you control the balance between **Performance, Accuracy, and Cost**.

=== Your Mission Log
This entire learning path is a series of missions. You have benchmarked, validated, and tuned. Now it's time for the most impactful optimization technique.

* **Mission 1: Benchmark the Battlefield with GuideLLM**
* **Mission 2: Validate Your Ally's Strength with lm-eval-harness**
* **Mission 3: Tune Your Engine with vLLM Optimization**
* **This Mission: Travel Light and Fast with LLM Compressor**
+
--
This is the art of making models smaller, faster, and cheaper to run. Your task is to use quantization to drastically reduce a model's size, answering the question: "How can I make this model radically more efficient?"
--

=== The Rules of Engagement
These labs are blueprints, not paint-by-numbers exercises. They are based on real-world patterns. Your challenge is to **adapt these blueprints** and think like an operator to make these tools work within your constraints to achieve your goals.

You're now ready for the final and most impactful mission. Let's get started.

// -- Page Break --

== Part 1: Hands-on Quantization in a Notebook

We'll begin by performing quantization manually in a Jupyter Notebook. This hands-on approach is the best way to understand the mechanics of the process before we move on to automation.

=== Module 1: Setting Up the Environment

First, we need to create a dedicated project and configure the necessary resources in OpenShift AI.

. **Create a Data Science Project**
+
Navigate to **Data Science Projects** in the OpenShift AI dashboard and create a new project named `quantization`.
+
[.bordershadow]
image::quant-create-project.png[Create Data Science Project, width=100%]

. **Create Data Connections**
+
Inside your `quantization` project, click **Add Data Connection**. You will create two S3-compatible connections to your MinIO storage.
* **Connection 1 (for pipelines):**
** **Name:** `pipeline-connection`
** **Bucket:** `pipelines`
** (Use your MinIO Access Key, Secret Key, and Endpoint)
* **Connection 2 (for models):**
** **Name:** `minio-models`
** **Bucket:** `models` (This bucket should contain the `ibm-granite/granite-3.3-2b-instruct` model)
+
[.bordershadow]
image::quant-data-connection.png[Data Connection Configuration, width=100%]

. **Configure a Pipeline Server**
+
Navigate to **Data science pipelines** -> **Pipelines** and click **Configure Pipeline Server**. Select the `pipeline-connection` data connection you just created.

. **Create and Launch a Workbench**
* In the `quantization` project, click **Create workbench**.
* Configure it with these settings:
** **Name**: `granite-quantization`
** **Image Selection**: `CUDA`
** **Container Size**: `Standard`
** **Accelerator**: `NVIDIA-GPU`
** **Number of accelerators**: `2`
* Under the **Connections** section, attach the existing **minio-models** data connection.
* Click **Create workbench** and wait for its status to show **Running**.

. **Clone the Lab Repository**
* Launch your running workbench.
* Inside the JupyterLab interface, use the Git extension to clone the following repository:
+
[source,sh]
----
https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git
----

Your environment is now fully prepared for the hands-on exercise.

// -- Page Break --

=== Module 2: Performing W4A16 Quantization

Now you'll perform the actual quantization using the provided Jupyter notebook.

. **Navigate and Open the Notebook**
+
In the JupyterLab file browser, navigate to the folder: `etx-llm-optimization-and-inference-leveraging/workshop_code/quantization/llm_compressor`.
+
Open the notebook named `weight_activation_quantization.ipynb`.
+
[.bordershadow]
image::quantization-int8-notebook.png[Weight Activation Quantization Notebook, width=100%]

. **Execute the Notebook Cells**
+
Read through the notebook's markdown cells for explanations. Execute the code cells in order by selecting a cell and pressing **Shift + Enter**. The notebook will guide you through the complete process:
* **Loading** the pre-trained `granite-2b` model.
* **Preparing** a calibration dataset.
* **Applying** W4A16 quantization using SmoothQuant and GPTQ techniques.
* **Saving** the new, compressed model to storage.
* **Evaluating** the quantized model's accuracy to see the impact.

. **Stop the Workbench**
+
IMPORTANT: Once you have successfully completed the notebook, you must stop your workbench. This is a critical step to free up the expensive GPU resources, making them available for other tasks.
+
[.bordershadow]
image::quantization-notebook-workbench-stop.png[Stop Workbench to Free GPU Resources, width=100%]

// -- Page Break --

== Part 2: Automating Quantization with Pipelines

Performing quantization in a notebook is great for learning, but for repeatable, enterprise-grade workflows, automation is essential. In this part, we'll convert the manual process into an automated Kubeflow Pipeline.

=== Module 3: Building and Compiling the Pipeline

First, we'll use our workbench environment to compile the Python code that defines our pipeline.

. **Start Your Workbench and Install Dependencies**
+
**Start** your `granite-quantization` workbench again. Once it's running, open a **Terminal** from the JupyterLab launcher.
+
Install the Kubeflow Pipelines SDK by running the following command in the terminal:
+
[source,sh,role=execute]
----
pip install -U kfp==2.9.0 kfp-kubernetes==1.3.0
----

. **Compile the Pipeline Definition**
+
The file `quantization_pipeline.py` contains the definition for our automated workflow. It includes steps to download a model, quantize it, upload the result, and evaluate it.
+
In the terminal, navigate to the `.../llm_compressor` directory and run the compilation script:
+
[source,sh,role=execute]
----
python quantization_pipeline.py
----
+
This command creates a new file, `quantization_pipeline.yaml`. This file is the blueprint for our pipeline.

. **Download the Pipeline and Stop the Workbench**
+
From the JupyterLab file browser, right-click on `quantization_pipeline.yaml` and download it to your local machine.
+
Once again, **stop the workbench** to release the GPU resources.

// -- Page Break --

=== Module 4: Running the Automated Pipeline

Now we will use the OpenShift AI dashboard to import and run our compiled pipeline. This UI-driven approach is perfect for managing and monitoring operational workflows.

. **Import the Pipeline**
+
In your `quantization` project on the OpenShift AI dashboard, navigate to **Data Science Pipelines** -> **Pipelines**.
+
Click **Import Pipeline**, choose **Upload**, and select the `quantization_pipeline.yaml` file you just downloaded. Give it a descriptive name like `Model Quantization Pipeline`.
+
[.bordershadow]
image::quantization-import-pipeline-select.png[Upload Pipeline YAML File, width=100%]

. **Create and Start a Pipeline Run**
+
Once the pipeline is imported, find it in the list, click the **Actions** menu (three dots), and select **Create run**.
+
A form will appear with the pipeline's parameters. You can accept the defaults, which will perform `int4` quantization on the `granite-2b` model downloaded from S3.
+
[.bordershadow]
image::quantization-import-pipeline-create-run-params.png[Pipeline Run Parameters Configuration, width=100%]
+
Click **Create run** to start the execution.

. **Monitor and Verify the Results**
+
You can monitor the run's progress visually in the dashboard. Each step will turn green as it completes successfully.
+
[.bordershadow]
image::quantization-pipeline-run-success.png[Successful Pipeline Execution Status, width=100%]
+
After the pipeline finishes, navigate to your MinIO S3 browser. In the `models` bucket, you will find a new folder (e.g., `granite-int4-pipeline`) containing the compressed model files. You can verify the success of the quantization by noting the significantly smaller size of the `.safetensors` files.

[TIP]
====
For advanced automation, these pipelines can also be triggered via the Kubeflow Pipelines REST API, allowing integration into larger CI/CD systems. The UI-driven approach shown here is ideal for managing and monitoring individual runs.
====

// -- Page Break --

== Course Wrap-up

Congratulations! You have successfully quantized a Large Language Model both manually to understand the process and through an automated pipeline to prepare it for production.

=== What You've Accomplished

* **Hands-on Mastery**: You used `LLM Compressor` with advanced techniques like `SmoothQuant` and `GPTQ` to tangibly reduce a model's memory footprint.
* **Automation for Production**: You transformed a manual notebook process into a robust, repeatable Kubeflow Pipeline, a cornerstone of any MLOps practice.
* **Business Impact**: You have learned the single most effective technique for reducing the infrastructure cost of serving large models, making enterprise AI more economically viable.

You are now equipped with a powerful skill to help clients achieve 50-75% cost reductions while maintaining high model quality—a compelling value proposition for any AI initiative.